\chapter{Discussion}
\label{sec:discussion}

%The result presented in section \ref{sec:resultsimages} is an example how a standard format and protocol DICOM is utilized to integrate current production system in order to exchange medical images (MEDIMED \cite{Slavicek2010}) and a grid-based solution (Globus MEDICUS \cite{Erberich2007}). Remote Desktop Protocol (RDP) is a key standard for protocol in order to integrate the application of voice analysis\cite{Fric2007} into a remote environment, which is accessible via the Internet. This is presented in section \ref{sec:resultsvoice}. In the case of parameter estimation, a key factor is the standard Functional Mockup Interface (FMU)\cite{Blochwitza},  which allows the control and simulation of a physiological model in a customized tool that is not related to modeling tool. This is presented in section \ref{sec:resultsestimation}. 

%The selection of a joint element increases the chances of reusability of such a system in future development, when requirements usually change and the reconstruction of a system or architecture is needed. For example, 

The presented solution, which is based on Globus MEDICUS, is, in general, a data warehouse, that stores one or more copies of DICOM images. However, federated files and metadata that are stored within home institutions, which only share network infrastructure to interchange the DICOM studies, seems to be a preferred and more acceptable solution by hospitals today, as published by Chervenak et al. \cite{Chervenak2012}. The grid computing infrastructure seems to be suitable for research and educational purposes, but not generally acceptable for clinical use. 
 
In the case of remote voice analysis, the remote access to an application keeps the majority of user experience via network protocol, as presented in section \ref{sec:resultsvoice}. It is a way how to migrate legacy application into the computing infrastructure and how to offer it as a service via network protocols. Cloud computing allows to instantiate the virtual machine with such service on demand.

% Such service can be deployed on any web server and the occasional need to educate or perform a higher number of analysis concurrently can be satisfied with cloud computing deployment. 
%The application process for sound signal which is currently analyzed by Fast Fourier Transformation algorithm quite effectively. Another challenge is to analyze a sound signal connected with high-speed video or videokymography methods, which need to transfer, process and store larger amounts of data. 

In the case of the application for parameter estimation presented in section \ref{sec:resultsestimation}, the computation is sensitive on communication overhead. For simple models, local high performance computing (HPC) resources are most beneficial. For medium and highly complex models, the deployment of worker nodes into a cloud computing environment is worth considering. Another challenge is to estimate optimal size of population for genetic algorithm in order to optimize computational time and reduce suboptimal results, as proposed by Gotshall et al.  \cite{Gotshall2000}.

The parameter sweep problem is considered as embarrassingly parallel and highly suitable for high throughput computing (HTC), which is the main focus of current grid computing infrastructures. 

When porting an application to a grid environment, one of the important decision is the platform of the used system, which is sometimes hard to involve, e.g., within the thesis the were available code from a third party tool for the MS Windows platform only. This can determine the platform of the worker node and the virtualization - or, in the case of parameter estimation, cloud computing is utilized on a prepared platform. In the case of parameter sweep, a desktop grid computing BOINC worker and application for a MS Windows platform is prepared for volunteers with the compatible system. To utilize the service grid infrastructure, an export of the model into a FMU library and implementation of the wrapper service must be done in the grid computing platform, which is usually a Linux based system. An option can be to use WINE\footnote{\url{https://www.winehq.org/} WINE. Accessed March 2015} -- a compatibility layer that is capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, Mac OSX and BSD. This would allow utilizing traditional service grid infrastructure for, e.g., parameter sweep application having an advantage not to maintain the desktop grid BOINC server.

For smaller types of application and scientific community with their own tools, the question is, whether or not to invest on porting their tools to grid specific platform and parallel programming model. 
In the case of integrating with a service grid middleware or with desktop grid framework, expert knowledge is needed to configure and customize the system. This is the case for the sharing of medical images (section \ref{sec:resultsimages}) and for parameter estimation and parameter sweep, which was tried with the desktop grid approach - BOINC framework.% (section \ref{sec:resultsboinc}).

Virtualization facilitates the integration effort, as presented in the case of remote analysis of the human voice (section \ref{sec:resultsvoice}) and in the case of deployment of worker nodes in a cloud computing environment for parameter estimation (section \ref{sec:resultsestimation}). 

Based on previous results and ideas, the answer to the research questions can be formulated:
\begin{itemize} 
\item \emph{Is it beneficial to utilize grid computing and cloud computing technology for the processing of medical information and how?}

Grid computing and cloud computing can significantly speedup parameter study of medium and complex models in computational physiology. Such a speedup could influence its applicability in clinical use. %, it was shown that parameter study of the medium and complex models can highly benefit from grid-computing and cloud-computing environment. For the simple models only HPC resources with reduced communication overhead might be beneficial too. 
For the case of sharing and processing medical images or analysis of voice signals, grid computing or cloud computing introduces technology that facilitates cooperation among a community of users from different geographically dispersed areas and facilitates the sharing of large data sets.

\item \emph{What are the limitations of processing medical information in grid or cloud?}

Limitation are given by the effort needed to integrate or port an application carry out computation or share data. The cost of porting an application to cloud computing is reduced by virtualization technology, rather than to a grid computing environment, which needs additional work in order to adapt the application for a grid computing platform and API. 

The limitation are given by the theoretical features of algorithms too. Grid computing and cloud computing are not general solutions for hard problems (NP-complete problems). With connected with non-exact methods, a concurrent processing of many tasks may bring an acceptable non-exact solution.

\item \emph{How can the grid computing and cloud computing influence the direction of biomedical research?}

The fact that the computation or data are processed remotely is one of the paradigm shift. The data moves from files stored in some folder to elements or objects living somewhere on server or cloud which can be shared among researchers. 
 
% proposed a noninvasive method based on computational fluid dynamic and magnetic resonance imaging (MRI) to identify pressure difference in aorta for further hemodynamics analysis based on four element windkessel model. However, such type of studies usually focus on particular phenomenon and tries identify parameters of very simplified models that are currently known in systems biology domain. In the time of writing this thesis, 

%And the grid-based or  cloud-based medical data management solution can be used as technology and infrastructure to store and share the expected big ammount of raw scientific results, such as images or videos. 
%Several computational and storage demanding biomedical application were shown in animal models by Saritas et al.\cite{Saritas2013}. 
The research infrastructures, e.g. Integrated Structural Biology Infrastructure for Europe (INSTRUCT)\footnote{\url{https://www.structuralbiology.eu/} accessed March 2015}, European Life Science Infrastructure for Biological Information (ELIXIR)\footnote{\url{http://www.elixir-europe.org/} accessed March 2015},  European Biomedical Imaging Infrastructure (Euro-BioImaging)\footnote{\url{http://www.eurobioimaging.eu/} accessed March 2015} and others rely on grid-computing and cloud-computing infrastructures for science. The purpose of these initiatives is to understand high-level phenotypes from genomic, metabolomic, proteomic, imaging and other types of data. They also require multi-scale mathematical models and simulations, as noted e.g. by Hunter et al. \cite{Hunter2013} in his strategy for Virtual Physiological Human (VPH)\footnote{\url{http://www.vph-institute.org/} accessed March 2015}. 

The integration with multidimensional models of geometrical, mechanical properties and the time-dependence of the compartment's data, which is taken from medical and biological repositories, can highly improve complex models of human physiology which are based mainly on lumped-parameter approach. E.g. Itu et al. achieved parameter identification on simplified windkessel model of hemodynamics in order to study aortic coarctation, which is based on processing of MRI, and requires 6-8 minutes of computation time on a standard personal computer \cite{Itu2013}. One of the challenge of systems biology approach, as identified by Kohl et al. \cite{Kohl2010}, is to use multiparameter perturbation to identify the safe areas, e.g., for multitarget drug profile. The results presented in section \ref{sec:resultsestimation} shows that the parameter study can be done on much more complex models in a reasonable time. The computation is able to become practical for clinical and further research towards patient-specific health care, in silico trails and drug discovery. 
%The research infrastructures, e.g. Integrated Structural Biology Infrastructure for Europe (INSTRUCT)\footnote{\url{https://www.structuralbiology.eu/} accessed March 2015}, European Life Science Infrastructure for Biological Information (ELIXIR)\footnote{\url{http://www.elixir-europe.org/} accessed March 2015},  European Biomedical Imaging Infrastructure (Euro-BioImaging)\footnote{\url{http://www.eurobioimaging.eu/} accessed March 2015} and others, which technologically rely on grid-computing and cloud-computing infrastructures for science. The purpose of these initiatives is to understand high-level phenotypes from genomic, metabolomic, proteomic, imaging and other types of data. They also require multi-scale mathematical models and simulations, as noted e.g. by Hunter et al. \cite{Hunter2013} in his strategy for Virtual Physiological Human (VPH)\footnote{\url{http://www.vph-institute.org/} accessed March 2015}. 
%corrected April 3th
%The integration with multidimensional models of geometrical, mechanical properties and the time-dependence of the compartment's data, which is taken from medical and biological repositories, can highly improve complex models of human physiology which are based mainly on lumped-parameter approach. E.g. Itu et al. achieved parameter identification on simplified windkessel model of hemodynamics in order to study aortic coarctation, which is based on processing of MRI, and requires 6-8 minutes of computation time on a standard personal computer \cite{Itu2013}. On of the challenge of systems biology approach, as identified by Kohl et al. \cite{Kohl2010}, is to use multiparameter perturbation to identify the safe areas, e.g., for multitarget drug profile. The results presented in section \ref{sec:resultsestimation} shows that the parameter study can be done on much more complex models in a reasonable time. The computation is able to become practical for clinical and further research towards patient-specific health care, in silico trails and drug discovery. 

%Additionally, it is a business strategy of several new ventures in order to collect anonymized patient records from clinicians, to gradually improve diagnostic methods and to provide reciprocal services for supporting clinical or therapeutic decision, as presented in section \ref{sec:resultsvoice}. For example,  Fetview\footnote{\url{http://fetview.com/} accessed March 2015} is an startup company, which one aim is to support fetal healthcare and gradually improve diagnostic methods, which is based on already collected records in history.

%providing access to advanced services and improved to 
%gaining access to advanced services to support clinical or therapeutist decision becomes more widely adopted by companies utilizing cloud-computing with heavy encryption mechanism, at the same time the database of anonymized records from history gradually improve algorithms for further diagnostics. E.g. Fetview\footnote{\url{http://fetview.com/} accessed March 2015} is an startup company to support such use cases in fetal healthcare.


%Some of the methods and pilot results were shown in sections \ref{sec:resultsestimation} and \ref{sec:resultsboinc}. 
\end{itemize}

%\subsection{Long-tail of science}

Based on the previous answers, another research question can be formulated for further research in the technology domain: 

\emph{How can biomedical research influence the direction of grid-computing and cloud-computing development?}

One area of discussion about this theme is how to preserve scientific data in long term in order to prevent loss of them \cite{Vines2014,P.BryanHeidorn2008}. The provenance and reproducibility of scientific results implied a need to long-term preservation of scientific data, however, if it is left on individual researcher, there is loss of data, as analysed by Vines et al. or Heidorn \cite{Vines2014,P.BryanHeidorn2008}.

Another area of discussion is how to facilitate access to computational resources for large amounts of small scientific group, which have limited resources to port, integrate or customize their current tools and processes -- to support the "long-tail" of science. The "long-tail" movement was first noted and described by Anderson \cite{Anderson2006} in the business domain. % as a shift of business strategy focusing on offering and delivering not only very popular products but also products with relatively small quantities sold each to final consumers. 
The long-tail term comes from a feature of statistical distribution, e.g., pareto distribution, where only a few (e.g., 20\% -- noted as head) elements have a high probability of some events (e.g., product being sold), while the rest (e.g., 80\% -- noted as tail) have a small probability. Thus, most businesses focus on hits (20\% of products, the 80-20 rule). The expansion of the Internet and its related technologies have caused reduced sales, marketing and delivery costs for the products from the niche (80\% of products) -- long-tail. A strategy that focused on these kinds of products became profitable and successful, e.g., for companies such as Amazon or Apple.\cite{Anderson2006}. 
Cloud computing technologies seem to be customizable and may be an enabling technology to focus on long-tail science, 
as noted e.g. by Weinhardt et al. \cite{Weinhardt2009}. 

How to facilitate and decrease an effort to develop, customize and port domain-specific application to some distributed computing model? This problem motivated, 
e.g., Anjum et al. to establish "platform as a service" (category of cloud computing service model) integrating several grid computing and cloud computing standards glueing via service oriented architecture approach \cite{Anjum2012}. 
Complementary approach is to support consultation, training and exchange in research software development toward the domain scientists, e.g., as presented by Crouch et al. regarding the Software Sustainable Institute within United Kingdom \cite{Crouch2013}.
%The ideas above applies for potential influence and requirements given by any scientific domain.
% While infrastructure as a service approach delivered by cloud-computing gives a freedom to create virtual environment and deploy any type of application, Software as a service delivers usually solution for particular problem. The Platform as a service approach may be an challenge for further cloud-computing infrastructure development as it can deliver some kind of tool which is not solution, but can give a scientists a formal way to describe the problem, import data and visualize the processing of information using programming language capabilities but with strong tools and common workflows already implemented to address the long-tail of science.
%For the case of algorithms, that are already present in grid-computing middleware is key factor the worker/simulation part which is specific for each research community. 
%The infrastructure built for purpose of large projects becomes open for smaller teams of scientists who doesn't have strong IT department or background. The process introduced before might be inneficient in terms of researcher's time who may spent time to porting it's application and tunning it up in grid or cloud environment. 
%Integration to tools researchers already use, or introduce new tools with low learning curve ...
%It was also shown that the batch-processing is not appropriate for some sort of application. The voice analysis is expected to be done in real-time and with access to some additional software, services and databases which are hard to maintain locally. 
%\subsubsection{Long Tail Science}

%The provenance and reproducibility of scientific results implied a need to long-term preservation of scientific data, however, if it is left on individual researcher, there is loss of data, as analysed by Vines et al. or Heidorn \cite{Vines2014,P.BryanHeidorn2008}.

