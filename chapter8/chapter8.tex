\chapter{Conclusion}
\label{sec:conclusion}

\section{Discussion}
\label{sec:discussion}
%Important decision about utilizing distributed computing infrastructure is what benefits it'll bring with respect of the caveats and resources it'll consume including the human capital compared to other existing solution.

%\subsection{Medical imaging}
%There are several use cases where exchanging or sharing DICOM images are beneficial, next to the second opinion and teleradiology (remote diagnostics), gathering information and expertize in rare diseases, studies in unusual cases or secondary use of clinical imaging data for research, optimizing new processing methods etc. 

%The Globus MEDICUS stores one or more copies of DICOM images within grid infrastructure, and as such, can be used as technology and infrastructure to built data warehouse for DICOM images of specific interest. Another philosophy is to federate files and metadata stored within home institutions, which seems to be preferred solution in hospitals and clinical use which were shown in other medical sharing image projects.

The result presented in section \ref{sec:resultsimages} is an example how a standard format and protocol DICOM is utilized to integrate current production system in order to exchange medical images (MEDIMED \cite{Slavicek2010}) and a grid-based solution (Globus MEDICUS\cite{Erberich2007}). Remote Desktop Protocol (RDP) is a key standard for protocol in order to integrate the application of voice analysis\cite{Fric2007} into a remote environment, which is accessible via the Internet. This is presented in section \ref{sec:resultsvoice}. In the case of parameter estimation, a key factor is the standard Functional Mockup Interface (FMU)\cite{Blochwitza},  which allows the control and simulation of a physiological model in a customized tool that is not related to modeling tool. This is presented in section \ref{sec:resultsestimation}. 

The selection of a joint element increases the chances of reusability of such a system in future development, when requirements usually change and the reconstruction of a system or architecture is needed. For example, the presented solution, which is based on Globus MEDICUS, is, in general, a data warehouse, that stores one or more copies of DICOM images. However, federated files and metadata that are stored within home institutions, which only share network infrastructure to interchange the DICOM studies, seems to be a preferred and more acceptable solution by hospitals. Thus, in their further development, the authors of Globus MEDICUS followed a way of federation of medical images that are stored within home institutions, as published by Chervenak et al. \cite{Chervenak2012}. The grid computing infrastructure seems to be suitable for research and educational purposes, but not generally acceptable for clinical use. %Cloud-computing with ability of hybrid cloud, where part of virtual machines may reside in home institution may be also answer on this issue.

%here an underlying technology is hidden for common user. This research was originally motivated by the idea to investigate benefits and show robust grid-based technology against proprietary distributed technology, which may face up to scalability and maintenance issues. Another issue is the philosophy of storing medical images. The presented solution based on the Globus MEDICUS is in general a data warehouse storing one or more copies of DICOM images, in contrast to federated files and metadata stored within home institutions which shares only network infrastructure to interchange the DICOM studies. This seems to be more acceptable by hospitals and by the national and international regulation for clinical and diagnostic use, e.g. The authors of Globus MEDICUS in their further development followed a way of federation of medical images stored within home institutions rather than in a grid infrastructure published by Chervenak et al. \cite{Chervenak2012}. 
%

%However, the scalability is managed within the current MEDIMED system and separate PACS system including database of anonymized medical images with the medical context and description for research and education purposes coexist within the current system.


%This allows to keep current tools for processing the medical images when the technology of infrastructure is changed.
%Thus user experience is kept on using same tools or same workflows, however, accessing much broader database of records or having more powerful tools for processing the data.
 
In the case of remote voice analysis, the remote access to an legacy application via network protocol keeps the majority of user experience, as presented in section \ref{sec:resultsvoice}. Such service can be deployed on any web server and the occasional need to educate or perform a higher number of analysis concurrently can be satisfied with cloud computing deployment. The application process for sound signal which is currently analyzed by Fast Fourier Transformation algorithm quite effectively. Another challenge is to analyze a sound signal connected with high-speed video or videokymography methods. This need however to transfer, process and store large amounts of data (GB). With current common network speed available in departments of laryngology (up to one GBit) this may introduce impractical latency needed to transfer data to remote application and is not suitable for real-time analysis compared to analysis of sound signal. But for post-processing and storing such records, this system can bring benefits for second study, gradual improvement of diagnostic methods, etc.  
%added 25th March
%Gaining access to advanced services to support clinical or therapeutist decision becomes more widely adopted by companies utilizing cloud-computing with heavy encryption mechanism, at the same time the database of anonymized records from history gradually improve algorithms for further diagnostics. E.g. Fetview\footnote{\url{http://fetview.com/} accessed March 2015} is an startup company to support such use cases in fetal healthcare.
%Such future application might utilize the results of grid-based systems for sharing medical images.

In the case of the application for parameter estimation presented in section \ref{sec:resultsestimation}, the computation is sensitive on communication overhead. For simple models, local high performance computing (HPC) resources are most beneficial. For medium and highly complex models, the deployment of worker nodes into a cloud computing environment is worth considering. Another challenge is an optimal size of population for genetic algorithm so the algorithm will converge to some acceptable solution in a reasonable time, as Gotshall et al. proposed a method for determining the optimum population size for a given problem \cite{Gotshall2000}.

%added 25th March, compare with existing publication 
The parameter sweep problem is considered as embarrasingly parallel and highly suitable for high throughput computing (HTC), which is the main focus of current grid computing infrastructures. TÃ¸ndel et al. introduced methods to statistically map variation of large number of parameters and to reduce drastically the number of simulations required for  parameter study \cite{Tondel2011}, however only non-complex models focusing on specific phenomenon were considered.

%Magnetic Particle Imager \footnote{introduced first by Gleich and Weizenecker\cite{Gleich2005}} (available only for animal models and not yet for human medicine) can produce high resolution images with fast shutter speed (~20 ms). Several computational and storage demanding biomedical application were shown in animal models by Saritas et al.\cite{Saritas2013}. There are research infrastructures which were established to coordinate the research in biology and medicine, e.g. Integrated Structural Biology Infrastructure for Europe (INSTRUCT)\footnote{\url{https://www.structuralbiology.eu/} accessed March 2015}, European Life Science Infrastructure for Biological Information (ELIXIR)\footnote{\url{http://www.elixir-europe.org/} accessed March 2015},  European Biomedical Imaging Infrastructure (Euro-BioImaging)\footnote{\url{http://www.eurobioimaging.eu/} accessed March 2015} and others. The purpose of these initiatives is to understand high-level phenotypes from genomic, metabolomic, proteomic, imaging and other types of data and they requires also multiscale mathematical models and simulation. As noted by Hunter et al.\cite{Hunter2013} such mathematical models can be delivered by physiome projects e.g. by virtual physiological human (VPH)\footnote{\url{http://www.vph-institute.org/} accessed March 2015}. 
%The medical imaging and processing methods are used to identify the parameters of models of human physiology for further diagnosis statement and treatment decision. Current studies focus on very simplified models and reduced number of parameters, e.g. Ralovich et al.\cite{Ralovich2012}
  
%The added value of the grid-computing or cloud-computing technology is the ability of enhanced collaboration in use cases which were difficult to achieve, e.g. in cases if secondary use of clinical imaging data for research

%for is an important area in research of rare diseases. As seen from the results obtained either in previous chapter or by other authors, the grid-computing is only one technology to facilitate some interconection of storage and computation capacity of different sites and servers owned by different organizations or individuals. 

%As seen from the results, there are several main purpose for which the grid-computing or cloud-computing infrastructure was built and are currently used for:
%\begin{enumerate}
%\item{solve big computational problems in distributed environment and achieve results in a reasonable computational time}
%\item{facilitate processing of the computation to support processing, analysis for further scientific results. This is provided by scientific portals, services, workflows etc. which allows non-exports in grid-computing or cloud-computing to do their job reducing researcher's time.}
%\item{share and archive database of scientific data for further processing and reproducibility.}
%\end{enumerate}
%\subsubsection{Scalability}
%
%From the perspective of efficiency (time complexity) and scalability (degree of parallelization) the heuristic methods for parameter estimation application brings at least some solution in a reasonable time. For small models it, however, doesn't make sense as the overhead and parallelizable fraction limits the speedup gain from distribution to grid or cloud-computing\ref{sec:resultsestimation}. 
%
%For complex models the infrastructure of grid-computing or cloud-computing can be used to distribute simulation tasks to explore wider space of possible solution.

%\subsubsection{Platform}

When porting an application to a grid environment, one of the important decision to consider is the platform of the used system. The architecture, which involves computational nodes that are deployed in a cloud computing infrastructure is influenced by the fact that the model implementation is exported from a third party tool to the standard FMU library for the MS Windows platform, as mentioned in section \ref{sec:methodsestimation}. This determines the platform of the worker node and the virtualization - or, in the case of parameter estimation, cloud computing is utilized on a prepared platform with a MS Windows license. In the case of parameter sweep, a desktop grid computing BOINC worker and application for a MS Windows platform is only prepared for volunteers with the compatible system. To utilize the service grid infrastructure, an export of the model into a FMU library and implementation of the wrapper service must be done in the grid computing platform,  which is usually a Linux based system. Another option is to use WINE\footnote{\url{https://www.winehq.org/} WINE. Accessed March 2015} -- a compatibility layer that is capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, Mac OSX and BSD. 

%With respect to scalability the degree and fraction of paralelization from the whole computational process needs to be considered, as, e.g. for simulating simple models and estimating the parameters takes couple of minutes to hours on single computer, which may be acceptable and deployment on grid computing will introduce overhead which degrades the benefits.

%\subsubsection{Porting}
%Each of the introduced systems and application was from it's beginning prepared for serial workflow. To achieve higher level of programming model,  some manual intervention is usually needed on the system or source code of application.

For smaller types of application and scientific community with their own tools, the question is, whether or not to invest on porting their tools to grid specific platform and parallel programming model. 
In the case of integrating with a service grid middleware or with desktop grid framework, expert knowledge is needed to configure and customize the system. This is the case for the sharing of medical images (section \ref{sec:resultsimages}) and for parameter estimation and parameter sweep, which was tried with the desktop grid approach - BOINC framework (section \ref{sec:resultsboinc}). %footnote{\url{http://boinc.berkeley.edu/} accessed March 2015}., although by using the wrapper application, no need to develop application code in C,C++ with desktop grid api. 
Virtualization facilitates the integration effort, as presented in the case of remote analysis of the human voice (section \ref{sec:resultsvoice}) and in thecase of deployment of worker nodes in a cloud computing environment for parameter estimation (section \ref{sec:resultsestimation}). % e.g. in the case of voice science, the analytical application was deployed on virtual machine and made available via remote desktop feature and future development was kept on the platform and environment familiar for the researchers. In the case of parameter estimation a worker nodes for parameter estimation a virtual machines within cloud-computing infrastructure, the similar approach is taken, the specific computational application which runs on local cluster can be strengthen by deploying multiple instances in virtual machines.

Based on previous results and ideas, the answer to the questions from the section \ref{sec:goal} can be formulated:
\begin{itemize} 
\item \emph{Is it beneficial to utilize grid computing and cloud computing technology for the processing of medical information and how?}

Grid computing and cloud computing may significantly speedup parameter study of medium and complex models in computational physiology. Such a speedup might influence its applicability in clinical use. %, it was shown that parameter study of the medium and complex models can highly benefit from grid-computing and cloud-computing environment. For the simple models only HPC resources with reduced communication overhead might be beneficial too. 
For the case of sharing and processing medical images or analysis of voice signals, grid computing or cloud computing introduces technology that facilitates cooperation among a community of users from different geographically dispersed areas and facilitates the sharing of large data sets.

\item \emph{What are the limitations of processing medical information in grid or cloud?}

Limitation are given by the effort needed to integrate or port an application carry out computation or share data. The cost of porting an application to cloud computing is reduced by virtualization technology, rather than to a grid computing environment, which needs additional work in order to adapt the application for a grid computing platform and API. 

From a programming model point of view, limitation are given by the theoretical features of algorithms and the problems to be solved. Grid computing and cloud computing are not general solutions for hard problems (NP-complete problems), as discussed in section \ref{sec:introcomplexity}. However, connected with non-exact methods, a concurrent processing of many tasks may bring an acceptable non-exact solution.

\item \emph{How can the grid computing and cloud computing influence the direction of biomedical research?}

%The fact that the computation or data are processed remotely is one of the paradigm shift. The data moves from files stored in some folder to elements or objects living somewhere on server or cloud which can be shared among researchers. 
 
% proposed a noninvasive method based on computational fluid dynamic and magnetic resonance imaging (MRI) to identify pressure difference in aorta for further hemodynamics analysis based on four element windkessel model. However, such type of studies usually focus on particular phenomenon and tries identify parameters of very simplified models that are currently known in systems biology domain. In the time of writing this thesis, 

%And the grid-based or  cloud-based medical data management solution can be used as technology and infrastructure to store and share the expected big ammount of raw scientific results, such as images or videos. 
%Several computational and storage demanding biomedical application were shown in animal models by Saritas et al.\cite{Saritas2013}. 
One of the concrete results presented in previous sections is the virtual infrastructure established within local institution. The virtualization allows to consolidate and utilize current available resources and may be the first step towards more powerful infrastructures for grid and cloud computing.

The research infrastructures, e.g. Integrated Structural Biology Infrastructure for Europe (INSTRUCT)\footnote{\url{https://www.structuralbiology.eu/} accessed March 2015}, European Life Science Infrastructure for Biological Information (ELIXIR)\footnote{\url{http://www.elixir-europe.org/} accessed March 2015},  European Biomedical Imaging Infrastructure (Euro-BioImaging)\footnote{\url{http://www.eurobioimaging.eu/} accessed March 2015} and others rely on grid-computing and cloud-computing infrastructures for science. The purpose of these initiatives is to understand high-level phenotypes from genomic, metabolomic, proteomic, imaging and other types of data. They also require multi-scale mathematical models and simulations, as noted e.g. by Hunter et al. \cite{Hunter2013} in his strategy for Virtual Physiological Human (VPH)\footnote{\url{http://www.vph-institute.org/} accessed March 2015}. 
%corrected April 3th
The integration with multidimensional models of geometrical, mechanical properties and the time-dependence of the compartment's data, which is taken from medical and biological repositories, can highly improve complex models of human physiology which are based mainly on lumped-parameter approach. E.g. Itu et al. achieved parameter identification on simplified windkessel model of hemodynamics in order to study aortic coarctation, which is based on processing of MRI, and requires 6-8 minutes of computation time on a standard personal computer \cite{Itu2013}. On of the challenge of systems biology approach, as identified by Kohl et al. \cite{Kohl2010}, is to use multiparameter perturbation to identify the safe areas, e.g., for multitarget drug profile. The results presented in section \ref{sec:resultsestimation} shows that the parameter study can be done on much more complex models in a reasonable time. The computation is able to become practical for clinical and further research towards patient-specific health care, in silico trails and drug discovery. 

Additionally, it is a business strategy of several new ventures in order to collect anonymized patient records from clinicians, to gradually improve diagnostic methods and to provide reciprocal services for supporting clinical or therapeutic decision, as presented in section \ref{sec:resultsvoice}. For example,  Fetview\footnote{\url{http://fetview.com/} accessed March 2015} is an startup company, which one aim is to support fetal healthcare and gradually improve diagnostic methods, which is based on already collected records in history.

%providing access to advanced services and improved to 
%gaining access to advanced services to support clinical or therapeutist decision becomes more widely adopted by companies utilizing cloud-computing with heavy encryption mechanism, at the same time the database of anonymized records from history gradually improve algorithms for further diagnostics. E.g. Fetview\footnote{\url{http://fetview.com/} accessed March 2015} is an startup company to support such use cases in fetal healthcare.


%Some of the methods and pilot results were shown in sections \ref{sec:resultsestimation} and \ref{sec:resultsboinc}. 
\end{itemize}

%\subsection{Long-tail of science}

Based on the previous answers, another research question can be formulated for further research in the technology domain: 

\emph{How can biomedical research influence the direction of grid-computing and cloud-computing development?}

One area of discussion about this theme is how to preserve scientific data in long term in order to prevent loss of them \cite{Vines2014,P.BryanHeidorn2008}. Within the domain of computational physiology, this can be addressed in future, e.g., by the established repository of physiological values at \url{http://www.physiovalues.org}, keeping the values related to the model implementation and selected scenario. The challenges towards the grid and cloud could be the question about hosting of such type service and other specific requirements. 

Another area of discussion is how to facilitate access to computational resources for large amounts of small scientific group, which have limited resources to port, integrate or customize their current tools and processes -- to support the "long-tail" of science. The "long-tail" movement was first noted and described by Anderson \cite{Anderson2006} in the business domain. % as a shift of business strategy focusing on offering and delivering not only very popular products but also products with relatively small quantities sold each to final consumers. 
The long-tail term comes from a feature of statistical distribution, e.g., pareto distribution, where only a few (e.g., 20\% -- noted as head) elements have a high probability of some events (e.g., product being sold), while the rest (e.g., 80\% -- noted as tail) have a small probability. Thus, most businesses focus on hits (20\% of products, the 80-20 rule). The expansion of the Internet and its related technologies have caused reduced sales, marketing and delivery costs for the products from the niche (80\% of products) -- long-tail. A strategy that focused on these kinds of products became profitable and successful, e.g., for companies such as Amazon or Apple.\cite{Anderson2006}. 

Cloud computing technologies seem to be customizable and may be an enabling technology to focus on long-tail science, as noted e.g. by Weinhardt et al. \cite{Weinhardt2009}. How to facilitate and decrease an effort to develop, customize and port domain-specific application to some distributed computing model? This problem motivated, e.g., Anjum et al. to establish "platform as a service" (category of cloud computing service model) integrating several grid computing and cloud computing standards glueing via service oriented architecture approach \cite{Anjum2012}. Complementary approach is to support consultation, training and exchange in research software development toward the domain scientists, e.g., as presented by Crouch et al. regarding the Software Sustainable Institute within United Kingdom \cite{Crouch2013}.
%The ideas above applies for potential influence and requirements given by any scientific domain.
% While infrastructure as a service approach delivered by cloud-computing gives a freedom to create virtual environment and deploy any type of application, Software as a service delivers usually solution for particular problem. The Platform as a service approach may be an challenge for further cloud-computing infrastructure development as it can deliver some kind of tool which is not solution, but can give a scientists a formal way to describe the problem, import data and visualize the processing of information using programming language capabilities but with strong tools and common workflows already implemented to address the long-tail of science.
%For the case of algorithms, that are already present in grid-computing middleware is key factor the worker/simulation part which is specific for each research community. 
%The infrastructure built for purpose of large projects becomes open for smaller teams of scientists who doesn't have strong IT department or background. The process introduced before might be inneficient in terms of researcher's time who may spent time to porting it's application and tunning it up in grid or cloud environment. 
%Integration to tools researchers already use, or introduce new tools with low learning curve ...
%It was also shown that the batch-processing is not appropriate for some sort of application. The voice analysis is expected to be done in real-time and with access to some additional software, services and databases which are hard to maintain locally. 
%\subsubsection{Long Tail Science}

%The provenance and reproducibility of scientific results implied a need to long-term preservation of scientific data, however, if it is left on individual researcher, there is loss of data, as analysed by Vines et al. or Heidorn \cite{Vines2014,P.BryanHeidorn2008}.


\section{Summary}

This thesis presents the infrastructure, which, thanks to virtualization technology, joined several domain-specific tools in the field of sharing and processing medical images, performing real-time voice analysis and simulating human physiology. Theoretical aspects of computational complexity and parallelism were discussed within use cases of specific biomedical research problems.  

A seamless integration of grid-based PACS system was established with the current distributed system in order to share DICOM medical images. Access to real-time voice analysis application via remote desktop technology brings this type of service to any computer that can connect to the Internet. A system and portal to support the analysis and building of complex models of human physiology in the phase of parameter estimation and parameter sweep was introduced. Furthermore, additional computational nodes can be flexibly joined by starting the prepared virtual machines in cloud computing deployment. The methodology of building complex models of human physiology was contributed with the use of acausal and object-oriented modeling techniques. Methods for conducting a parameter study were shown, as well as the parameter study of complex models that gain substantial speedup by utilizing cloud computing deployment, which makes such kinds of complex studies applicable in physiological and biological research.
